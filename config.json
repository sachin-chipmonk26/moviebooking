{
    "architectures": [
      "LlamaForCausalLM"
    ],
    "model_type": "llama",
    "tokenizer_class": "LlamaTokenizer",
  
   
    "vocab_size": 32000,
    "hidden_size": 8192,
    "num_attention_heads": 64,
    "num_hidden_layers": 54

  }
  